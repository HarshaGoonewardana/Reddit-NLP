{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stops = set(stopwords.words('english'))\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15859, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df03=pd.read_csv('03-06-2018.csv')\n",
    "df02=pd.read_csv('02-06-2018.csv')\n",
    "\n",
    "df01=pd.read_csv('01-06-2018.csv')\n",
    "\n",
    "df_jun=pd.concat([df01, df02,df03])\n",
    "\n",
    "df_jun.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10547, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df31=pd.read_csv('31-05-2018.csv')\n",
    "\n",
    "df30=pd.read_csv('30-05-2018.csv')\n",
    "\n",
    "df_may=pd.concat([df30, df31])\n",
    "\n",
    "df_may.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform EDA**\n",
    "1. Drop duplicates within each dataframe and then combine them to form a new dataframe\n",
    "2. Convert the following temporal transformations:\n",
    "    a. Format the submission & retrieval times\n",
    "    b. Calculate the durations of posts by subtrating the retrieval time from the time posted\n",
    "    c. Format series for day and hour posted\n",
    "3. Calculate the number of words in each submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb_eda_df(df1,df2):\n",
    "    import datetime as dt\n",
    "    def get_day(date):\n",
    "        return time.strftime('%a')\n",
    "    def get_hm(time):\n",
    "        return time.strftime('%H')\n",
    "    \n",
    "    print(df1.shape)\n",
    "    print(df1.id.nunique())\n",
    "    print(df1.subreddit.value_counts())\n",
    "    df1=df1.drop_duplicates(subset='id',keep=\"last\")\n",
    "    print(df1.id.nunique())\n",
    "    print('')\n",
    "    \n",
    "\n",
    "    print(df2.shape)\n",
    "    print(df2.id.nunique())\n",
    "    print(df2.subreddit.value_counts())\n",
    "    df2=df2.drop_duplicates(subset='id',keep=\"last\")\n",
    "    print(df2.id.nunique())\n",
    "    print('')\n",
    "    \n",
    "    df_new = pd.concat([df1, df2])\n",
    "    df_new=df_new.drop_duplicates(subset='id',keep=\"last\")\n",
    "    df_new['Time'] = pd.to_datetime(df_new.created, unit='s')\n",
    "    df_new['Time_Retrieved'] = pd.to_datetime(df_new.time_retrieved)\n",
    "    df_new['Post_Duration']=(df_new['Time_Retrieved']-df_new['Time'])\n",
    "    df_new['Post_Duration']=(df_new['Post_Duration'].dt.seconds/60/60).round(0)\n",
    "    df_new['Time_HM']= df_new[\"Time\"].apply(get_hm)\n",
    "    df_new['Day'] =df_new[\"Time\"].apply(get_day)\n",
    "    df_new['Word_Count'] = [len(x.split()) for x in df_new['title'].tolist()]\n",
    "    print(df_new.id.nunique())\n",
    "    print(df_new.info())\n",
    "    return(df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine dataframes and drop duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10547, 9)\n",
      "5593\n",
      "politics        3733\n",
      "Conservative    3558\n",
      "Liberal         3256\n",
      "Name: subreddit, dtype: int64\n",
      "5593\n",
      "\n",
      "(15859, 9)\n",
      "6311\n",
      "politics        5667\n",
      "Conservative    5287\n",
      "Liberal         4905\n",
      "Name: subreddit, dtype: int64\n",
      "6311\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/envs/dsi/lib/python3.6/site-packages/ipykernel_launcher.py:23: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7224\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7224 entries, 12 to 5278\n",
      "Data columns (total 15 columns):\n",
      "body              147 non-null object\n",
      "comms_num         7224 non-null int64\n",
      "created           7224 non-null float64\n",
      "domain            7224 non-null object\n",
      "id                7224 non-null object\n",
      "score             7224 non-null int64\n",
      "subreddit         7224 non-null object\n",
      "time_retrieved    7224 non-null object\n",
      "title             7224 non-null object\n",
      "Time              7224 non-null datetime64[ns]\n",
      "Time_Retrieved    7224 non-null datetime64[ns]\n",
      "Post_Duration     7224 non-null float64\n",
      "Time_HM           7224 non-null object\n",
      "Day               7224 non-null object\n",
      "Word_Count        7224 non-null int64\n",
      "dtypes: datetime64[ns](2), float64(2), int64(3), object(8)\n",
      "memory usage: 903.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df=comb_eda_df(df_may,df_jun\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "politics        3382\n",
       "Conservative    2184\n",
       "Liberal         1658\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.subreddit.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate the politics subreddits from the Conservative & Liberals ones\n",
    "df_lib_con=df[(df['subreddit']=='Liberal') | (df['subreddit']=='Conservative')] \n",
    "df_pol=df[(df['subreddit']=='politics')]\n",
    "# OR \n",
    "# movies[(movies.duration >= 200) | (movies.genre == 'Drama')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_dfs(df):\n",
    "    TodaysDate = time.strftime(\"%d-%m-%Y\")\n",
    "    csv_name = TodaysDate +\".csv\"\n",
    "    df_new.to_csv(csv_name,index=False)\n",
    "    print(csv_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
